# ğŸ§  Offline LLM-Powered PDF Assistant

> A 100% offline, ChatGPT-style question-answering app for any PDF using **Ollama (Mistral)**, **FAISS**, **HuggingFace Embeddings**, and **Streamlit** â€” now with built-in LLM performance metrics.

---

## ğŸ” What It Does

- Upload any PDF (`data/sample.pdf`)
- Ask any question about the PDF
- Uses **Mistral 7B** running locally via **Ollama**
- Embeds content with **sentence-transformers**
- Finds relevant context using **FAISS vector search**
- Responds in full paragraphs â€” just like ChatGPT â€” but offline
- ğŸ“Š Tracks **real-time performance metrics** for the LLM

---

## ğŸ“Š What Metrics Are Tracked

Each time a question is asked, the app shows:

| Metric             | Description                                  |
|--------------------|----------------------------------------------|
| â±ï¸ Inference Time  | Time taken for the LLM to generate a response |
| ğŸ§  LLM CPU Usage   | CPU % used by the Ollama process              |
| ğŸ’¾ LLM RAM Usage   | RAM used by the LLM (RSS memory in MB)        |

---

## ğŸ“¦ Requirements

- Python 3.9 or above
- `pip install -r requirements.txt`
- [Install Ollama](https://ollama.com/download) and run:
  ```bash
  ollama run mistral

ğŸ“„ Sample PDF Included
Weâ€™ve included the official LLM Technical Report (GPT-4) as a sample PDF:

ğŸ“˜ GPT-4 Technical Report (arXiv)
ğŸ“ Located in: data/sample.pdf

ğŸ¤– Try Prompts Like:
â€œWhat are the key differences between GPT-3.5 and GPT-4?â€

â€œMention benchmark scoresâ€

â€œLimitations of GPT-4?â€

â€œSummarize the performance of GPT-4 on academic tasksâ€

â€œWhat tests were used to evaluate GPT-4?â€

â€œDoes GPT-4 outperform humans?â€

ğŸ–¥ï¸ Run Locally (Offline, No API Keys)
1. Clone the repository
bash
Copy
Edit
git clone https://github.com/YOUR_USERNAME/llm_research_assistant.git
cd llm_research_assistant
2. Create a virtual environment
bash
Copy
Edit
python -m venv venv
venv\Scripts\activate  # On Windows
3. Install Python dependencies
bash
Copy
Edit
pip install -r requirements.txt
4. Download & run the LLM via Ollama
bash
Copy
Edit
ollama run mistral
First-time download may take a few GB. Once installed, it runs fully offline.

5. Ingest the PDF into FAISS
bash
Copy
Edit
python ingest.py
This converts your PDF into vector chunks for search.

6. Launch the app
bash
Copy
Edit
streamlit run app.py
Visit your assistant at: http://localhost:8501

ğŸ“ Folder Structure
pgsql
Copy
Edit
llm_research_assistant/
â”œâ”€â”€ app.py                â† Streamlit UI with real-time LLM metrics
â”œâ”€â”€ ingest.py             â† PDF processing & vectorstore creation
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ .gitignore
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample.pdf        â† GPT-4 Technical Report (sample PDF)
â”œâ”€â”€ vectorstore/          â† FAISS DB (auto-generated)
ğŸ” Why Offline?
âœ… No API keys needed (100% local)

âœ… Works without internet once setup is complete

âœ… Keeps your documents private and secure

âœ… Useful for sensitive research, reports, or legal files

ğŸ‘¨â€ğŸ’» Author
Built with ğŸ’» by Vishal Chauhan

Star the repo â­ if you found it helpful â€” open to contributors!

yaml
Copy
Edit

---

Let me know if you want:
- A badge header (e.g. `made-with-python`, `offline-ai`, etc.)
- Screenshots or a GIF added to this
- A separate `docs/` folder with usage guide